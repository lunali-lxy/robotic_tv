#!/usr/bin/python3
# coding=utf8
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'  # è§£å†³OpenMPå†²çª

import cv2
import numpy as np
import json
import socket
import time
import onnxruntime as ort
import math
from PIL import Image, ImageDraw, ImageFont
import torch
import torch.nn as nn
from torchvision import transforms, models
from torchvision.transforms import ToTensor
from collections import deque
import threading
from queue import Queue

# é…ç½®å‚æ•°
MODEL_PATH = "model2.onnx"  # RT-DETRæ¨¡å‹è·¯å¾„
SERVER_IP = "192.168.51.180"  # æ ‘è“æ´¾IPåœ°å€
SERVER_PORT = 6000  # æ ‘è“æ´¾ç«¯å£
CAMERA_INDEX = 1  # æ‘„åƒå¤´ç´¢å¼•
CONF_THRESHOLD = 0.6  # ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä¸å‚è€ƒä»£ç ä¸€è‡´
IMAGE_WIDTH = 640  # å›¾åƒå®½åº¦
IMAGE_HEIGHT = 480  # å›¾åƒé«˜åº¦
FLIP_CAMERA = True  # æ˜¯å¦ç¿»è½¬æ‘„åƒå¤´ç”»é¢
INPUT_SIZE = (640, 640)  # æ¨¡å‹è¾“å…¥å°ºå¯¸ (W, H)

# å±å¹•çŠ¶æ€åˆ†ç±»æ¨¡å‹é…ç½®
CLASSIFICATION_WEIGHTS = "Last_Epoch016.pth"  # åˆ†ç±»æ¨¡å‹æƒé‡è·¯å¾„
CLASSIFICATION_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 8ä¸ªç±»åˆ«å®šä¹‰ - æŒ‰ç…§æ‚¨æä¾›çš„æ˜ å°„
CLASS_MAP = {0: "red", 1: "green", 2: "black", 3: "white",
             4: "pink", 5: "blue", 6: "fault", 7: "snow"}

# æœºæ¢°è‡‚æ§åˆ¶å‚æ•° - ä¿®æ­£æ–¹å‘
SEND_INTERVAL = 0.5  # å‘é€é—´éš”
MOVE_THRESHOLD = 20  # åƒç´ åç§»é˜ˆå€¼

# è¿åŠ¨å¹³æ»‘å‚æ•°
SMOOTHING_FACTOR = 0.7
MAX_HISTORY = 5

# æœç´¢æ¨¡å¼å‚æ•°
SEARCH_MOVE_RANGE = 10
SEARCH_MOVE_SPEED = 0.2
RESET_WAIT_TIME = 3.0

# åªå®šä¹‰ç”µè§†ç±»åˆ«
TV_CLASS = 0  # ç”µè§†æ•´ä½“

# ç®€åŒ–çš„å±å¹•çŠ¶æ€åˆ†ç±»é¢„å¤„ç†
classification_preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


class ScreenClassifier:
    def __init__(self, weights_path, device="cpu"):
        """åˆå§‹åŒ–å±å¹•çŠ¶æ€åˆ†ç±»å™¨"""
        self.device = device
        
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ¨¡å‹ç»“æ„
        self.model = models.efficientnet_b0(pretrained=False)
        num_features = self.model.classifier[1].in_features
        self.model.classifier[1] = nn.Linear(num_features, 8)
        self.model.to(self.device)
        self.model.eval()

        # åŠ è½½åˆ†ç±»æ¨¡å‹æƒé‡
        print(f"åŠ è½½åˆ†ç±»æ¨¡å‹æƒé‡: {weights_path}")
        try:
            if torch.cuda.is_available():
                checkpoint = torch.load(weights_path)
            else:
                checkpoint = torch.load(weights_path, map_location=torch.device('cpu'))

            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
            else:
                state_dict = checkpoint

            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}

            try:
                self.model.load_state_dict(state_dict)
                print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
            except RuntimeError as e:
                print(f"âŒ æƒé‡åŠ è½½é”™è¯¯: {e}")
                model_dict = self.model.state_dict()
                pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}
                model_dict.update(pretrained_dict)
                self.model.load_state_dict(model_dict)
                print(f"âœ… éƒ¨åˆ†æƒé‡åŠ è½½æˆåŠŸ: {len(pretrained_dict)}/{len(state_dict)} ä¸ªå‚æ•°")

            print(f"âœ… å±å¹•çŠ¶æ€åˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ åˆ†ç±»æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.model = None
    
    def preprocess_roi_fast(self, tv_roi):
        """å¿«é€Ÿé¢„å¤„ç†ç”µè§†ROIåŒºåŸŸ"""
        if tv_roi.size == 0:
            return None
            
        try:
            pil_img = Image.fromarray(cv2.cvtColor(tv_roi, cv2.COLOR_BGR2RGB))
            if pil_img.width < 20 or pil_img.height < 20:
                return None
            return pil_img
            
        except Exception as e:
            return None
    
    def analyze_screen_fast(self, tv_roi):
        """å¿«é€Ÿåˆ†æTVå±å¹•çŠ¶æ€"""
        if self.model is None or tv_roi is None or tv_roi.size == 0:
            return "waiting", 0.0

        try:
            pil_img = self.preprocess_roi_fast(tv_roi)
            if pil_img is None:
                return "waiting", 0.0
                
            img_tensor = classification_preprocess(pil_img).unsqueeze(0).to(self.device)

            with torch.no_grad():
                output = self.model(img_tensor)
                probs = torch.softmax(output, dim=1)
                pred_class = torch.argmax(probs, dim=1).item()
                pred_prob = probs[0, pred_class].item()

            if pred_class in CLASS_MAP:
                return CLASS_MAP[pred_class], pred_prob
            else:
                return f"class_{pred_class}", pred_prob
            
        except Exception as e:
            return "error", 0.0


class FrameDifferenceDetector:
    """åŸºäºå¸§é—´å·®å¼‚çš„å†…å®¹å˜åŒ–æ£€æµ‹å™¨"""
    def __init__(self, threshold=10, min_contour_area=500):
        self.threshold = threshold
        self.min_contour_area = min_contour_area
        self.prev_frame = None
    
    def detect_change(self, current_frame):
        """æ£€æµ‹å½“å‰å¸§ä¸å‰ä¸€å¸§çš„å†…å®¹å˜åŒ–"""
        if self.prev_frame is None:
            self.prev_frame = current_frame.copy()
            return True
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        prev_gray = cv2.cvtColor(self.prev_frame, cv2.COLOR_BGR2GRAY)
        current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # è®¡ç®—å¸§é—´å·®å¼‚
        frame_diff = cv2.absdiff(prev_gray, current_gray)
        
        # åº”ç”¨é˜ˆå€¼
        _, thresh = cv2.threshold(frame_diff, self.threshold, 255, cv2.THRESH_BINARY)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå¤§çš„å˜åŒ–åŒºåŸŸ
        content_changed = False
        for contour in contours:
            if cv2.contourArea(contour) > self.min_contour_area:
                content_changed = True
                break
        
        # æ›´æ–°å‰ä¸€å¸§
        self.prev_frame = current_frame.copy()
        
        return content_changed


class ClassificationWorker:
    """åˆ†ç±»å·¥ä½œçº¿ç¨‹"""
    def __init__(self, classifier):
        self.classifier = classifier
        self.task_queue = Queue(maxsize=1)
        self.result_queue = Queue(maxsize=1)
        self.worker_thread = threading.Thread(target=self._worker, daemon=True)
        self.running = True
        self.worker_thread.start()
    
    def _worker(self):
        while self.running:
            try:
                tv_roi = self.task_queue.get(block=False)
                if tv_roi is not None:
                    status, confidence = self.classifier.analyze_screen_fast(tv_roi)
                    if not self.result_queue.empty():
                        self.result_queue.get()
                    self.result_queue.put((status, confidence))
            except:
                time.sleep(0.001)
    
    def submit_task(self, tv_roi):
        try:
            if not self.task_queue.empty():
                self.task_queue.get(block=False)
            self.task_queue.put(tv_roi, block=False)
        except:
            pass
    
    def get_result(self):
        try:
            if not self.result_queue.empty():
                return self.result_queue.get(block=False)
        except:
            pass
        return None
    
    def stop(self):
        self.running = False
        if self.worker_thread.is_alive():
            self.worker_thread.join(timeout=1.0)


class TVDetector:
    def __init__(self, model_path):
        providers = ['CPUExecutionProvider']
        self.sess = ort.InferenceSession(model_path, providers=providers)
        print(f"âœ… ç”µè§†æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        
    def detect_tv(self, frame):
        """æ£€æµ‹ç”µè§†å±å¹•"""
        orig_h, orig_w = frame.shape[:2]
        
        im = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        im_resized = im.resize(INPUT_SIZE)
        im_data = ToTensor()(im_resized)[None]
        size_tensor = torch.tensor([[INPUT_SIZE[0], INPUT_SIZE[1]]])

        try:
            output = self.sess.run(
                output_names=None,
                input_feed={
                    'images': im_data.numpy(),
                    'orig_target_sizes': size_tensor.numpy()
                }
            )
        except Exception as e:
            print(f"âŒ æ¨ç†é”™è¯¯: {str(e)}")
            return np.array([]), np.array([]), np.array([])
            
        labels, boxes, scores = output

        scale_x = orig_w / INPUT_SIZE[0]
        scale_y = orig_h / INPUT_SIZE[1]

        valid_indices = scores[0] > CONF_THRESHOLD
        labels = labels[0][valid_indices]
        boxes = boxes[0][valid_indices]
        scores = scores[0][valid_indices]
        
        tv_indices = [i for i, label in enumerate(labels) if int(label) == TV_CLASS]
        labels = labels[tv_indices]
        boxes = boxes[tv_indices]
        scores = scores[tv_indices]
        
        scaled_boxes = []
        for b in boxes:
            x0, y0, x1, y1 = [float(coord) for coord in b]
            x0 *= scale_x
            x1 *= scale_x
            y0 *= scale_y
            y1 *= scale_y
            scaled_boxes.append([x0, y0, x1, y1])
        
        if len(scaled_boxes) > 0:
            scaled_boxes = np.array(scaled_boxes)
        else:
            scaled_boxes = np.array([])
            
        return scaled_boxes, labels, scores
    
    def calculate_tv_center(self, boxes, labels):
        """è®¡ç®—ç”µè§†ä¸­å¿ƒç‚¹å’Œå°ºå¯¸"""
        if len(boxes) == 0:
            return None
        
        largest_box = max(boxes, key=lambda box: (box[2]-box[0])*(box[3]-box[1]))
        
        center_x = (largest_box[0] + largest_box[2]) / 2
        center_y = (largest_box[1] + largest_box[3]) / 2
        width = largest_box[2] - largest_box[0]
        height = largest_box[3] - largest_box[1]
        
        return center_x, center_y, width, height


class CorrectArmController:
    """æ­£ç¡®æ–¹å‘çš„æœºæ¢°è‡‚æ§åˆ¶å™¨"""
    def __init__(self, server_ip, server_port):
        self.server_ip = server_ip
        self.server_port = server_port
        self.sock = None
        self.connected = False
        self.last_send_time = 0
        self.last_command = None
        
        self.connect_to_server()
    
    def connect_to_server(self):
        """è¿æ¥åˆ°æ ‘è“æ´¾æœåŠ¡å™¨"""
        try:
            if self.sock:
                try:
                    self.sock.close()
                except:
                    pass
                
            print(f"ğŸ”Œ å°è¯•è¿æ¥åˆ° {self.server_ip}:{self.server_port}...")
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.settimeout(3.0)
            self.sock.connect((self.server_ip, self.server_port))
            self.sock.settimeout(5.0)
            self.connected = True
            print(f"âœ… è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {str(e)}")
            self.connected = False
            return False
    
    def send_command(self, x, y, z):
        """å‘é€å‘½ä»¤åˆ°æœºæ¢°è‡‚"""
        current_time = time.time()
        
        # æ£€æŸ¥å‘é€é—´éš”
        if current_time - self.last_send_time < SEND_INTERVAL:
            return True
            
        if not self.connected:
            if not self.connect_to_server():
                return False
        
        command = {'x': round(x, 1), 'y': round(y, 1), 'z': round(z, 1)}
        
        try:
            # å‘é€å‘½ä»¤
            command_str = json.dumps(command)
            self.sock.sendall(command_str.encode('utf-8'))
            print(f"ğŸ¯ å‘é€: {command_str}")
            
            # å°è¯•æ¥æ”¶å“åº”
            self.sock.settimeout(1.0)
            try:
                response = self.sock.recv(128)
                if response:
                    response_str = response.decode().strip()
                    print(f"ğŸ“¥ å“åº”: {response_str}")
            except socket.timeout:
                pass
            except Exception as e:
                print(f"âš ï¸ è¯»å–å“åº”é”™è¯¯: {str(e)}")
            
            # æ¢å¤è¶…æ—¶è®¾ç½®
            self.sock.settimeout(5.0)
            
            self.last_send_time = current_time
            self.last_command = (x, y, z)
            return True
            
        except socket.timeout:
            print("âš ï¸ å‘é€è¶…æ—¶")
            self.connected = False
            return False
        except Exception as e:
            print(f"âŒ å‘é€å¤±è´¥: {str(e)}")
            self.connected = False
            return False
    
    def stop(self):
        """åœæ­¢æ§åˆ¶å™¨"""
        if self.sock:
            try:
                self.sock.close()
            except:
                pass


class TVTracker:
    def __init__(self, model_path, server_ip, server_port, classifier_weights):
        self.detector = TVDetector(model_path)
        self.classifier = ScreenClassifier(classifier_weights, CLASSIFICATION_DEVICE)
        self.classification_worker = ClassificationWorker(self.classifier)
        self.frame_difference_detector = FrameDifferenceDetector(threshold=15, min_contour_area=300)
        
        # ä½¿ç”¨æ­£ç¡®æ–¹å‘çš„æœºæ¢°è‡‚æ§åˆ¶å™¨
        self.arm_controller = CorrectArmController(server_ip, server_port)
        
        # ä½ç½®å†å²
        self.x_history = deque(maxlen=MAX_HISTORY)
        self.y_history = deque(maxlen=MAX_HISTORY)
        self.z_history = deque(maxlen=MAX_HISTORY)
        
        # ç›®æ ‡ä½ç½®
        self.target_x = 0
        self.target_y = 15
        self.target_z = 15
        
        # è¿è¡Œæ ‡å¿—
        self.running = True
        self.tv_detected = False
        self.last_tv_time = 0
        
        # æœç´¢æ¨¡å¼
        self.search_mode = False
        self.search_start_time = 0
        self.search_phase = 0
        
        # å±å¹•çŠ¶æ€
        self.screen_status = "waiting"
        self.screen_confidence = 0.0
        self.content_changed = False
        self.force_classification = True
        self.classification_count = 0
        
        # å¸§ç‡ç»Ÿè®¡
        self.frame_count = 0
        self.start_time = time.time()
        self.avg_fps = 0
        
        # å°è¯•åŠ è½½å­—ä½“
        try:
            self.font = ImageFont.truetype("arial.ttf", 16)
        except:
            self.font = None
    
    def calculate_target_position(self, center_x, center_y, tv_w, tv_h):
        """è®¡ç®—æœºæ¢°è‡‚ç›®æ ‡ä½ç½® """
        image_center_x = IMAGE_WIDTH / 2
        image_center_y = IMAGE_HEIGHT / 2
        
        offset_x = center_x - image_center_x
        offset_y = center_y - image_center_y
        
        # å¦‚æœåç§»é‡å°äºé˜ˆå€¼ï¼Œåˆ™æœºæ¢°è‡‚ä¸åŠ¨
        if abs(offset_x) < MOVE_THRESHOLD and abs(offset_y) < MOVE_THRESHOLD:
            return None, None, None
        
        # æ–¹å‘é€»è¾‘ï¼š
        # 1. ç”µè§†åœ¨ç”»é¢å·¦è¾¹ï¼ˆoffset_x < 0ï¼‰â†’ æœºæ¢°è‡‚å‘å·¦ç§»åŠ¨ï¼ˆå‡å°‘xï¼‰
        # 2. ç”µè§†åœ¨ç”»é¢å³è¾¹ï¼ˆoffset_x > 0ï¼‰â†’ æœºæ¢°è‡‚å‘å³ç§»åŠ¨ï¼ˆå¢åŠ xï¼‰
        # 3. ç”µè§†åœ¨ç”»é¢ä¸Šæ–¹ï¼ˆoffset_y < 0ï¼‰â†’ æœºæ¢°è‡‚å‘ä¸Šç§»åŠ¨ï¼ˆå¢åŠ zï¼‰
        # 4. ç”µè§†åœ¨ç”»é¢ä¸‹æ–¹ï¼ˆoffset_y > 0ï¼‰â†’ æœºæ¢°è‡‚å‘ä¸‹ç§»åŠ¨ï¼ˆå‡å°‘zï¼‰
        
        # å°†åç§»é‡è½¬æ¢ä¸ºæœºæ¢°è‡‚ç§»åŠ¨é‡
        # Xè½´: ç”µè§†ä¸­å¿ƒåå·¦ï¼Œæœºæ¢°è‡‚éœ€è¦å‘å·¦ç§»åŠ¨ï¼ˆå‡å°xï¼‰
        #       ç”µè§†ä¸­å¿ƒåå³ï¼Œæœºæ¢°è‡‚éœ€è¦å‘å³ç§»åŠ¨ï¼ˆå¢åŠ xï¼‰
        x_adjust = offset_x / image_center_x * 4  # ç¼©æ”¾å› å­ï¼Œä¿æŒæ­£å€¼
        
        # Zè½´: ç”µè§†ä¸­å¿ƒåä¸Šï¼Œæœºæ¢°è‡‚éœ€è¦å‘ä¸Šç§»åŠ¨ï¼ˆå¢åŠ zï¼‰
        #       ç”µè§†ä¸­å¿ƒåä¸‹ï¼Œæœºæ¢°è‡‚éœ€è¦å‘ä¸‹ç§»åŠ¨ï¼ˆå‡å°‘zï¼‰
        z_adjust = offset_y / image_center_y * 3  # ç¼©æ”¾å› å­ï¼Œä¿æŒæ­£å€¼
        
        # Yè½´: æ ¹æ®ç”µè§†å¤§å°è°ƒæ•´è·ç¦»
        #       ç”µè§†å¤ªå¤§ â†’ åé€€ï¼ˆå¢åŠ yï¼‰
        #       ç”µè§†å¤ªå° â†’ å‰è¿›ï¼ˆå‡å°‘yï¼‰
        # æ³¨æ„ï¼šyè½´æ˜¯è·ç¦»ï¼Œå¢åŠ yè¡¨ç¤ºåé€€ï¼Œå‡å°‘yè¡¨ç¤ºå‰è¿›
        tv_size_ratio = tv_w / IMAGE_WIDTH
        if tv_size_ratio > 0.4:  # ç”µè§†å¤ªå¤§ï¼Œéœ€è¦åé€€
            y_adjust = 2
        elif tv_size_ratio < 0.2:  # ç”µè§†å¤ªå°ï¼Œéœ€è¦å‰è¿›
            y_adjust = -2
        else:
            y_adjust = 0
        
        # è®¡ç®—æ–°çš„ç›®æ ‡ä½ç½®
        new_x = self.target_x + x_adjust
        new_y = self.target_y + y_adjust
        new_z = self.target_z - z_adjust  # æ³¨æ„ï¼šzè½´æ–¹å‘éœ€è¦å–åï¼Œå› ä¸ºå›¾åƒåæ ‡yå‘ä¸‹ä¸ºæ­£
        
        # é™åˆ¶æœºæ¢°è‡‚è¿åŠ¨èŒƒå›´
        new_x = max(-20, min(20, new_x))
        new_y = max(10, min(25, new_y))
        new_z = max(10, min(20, new_z))
        
        return new_x, new_y, new_z
    
    def smooth_position(self, x, y, z):
        """å¹³æ»‘ä½ç½®å˜åŒ–"""
        self.x_history.append(x)
        self.y_history.append(y)
        self.z_history.append(z)

        if len(self.x_history) == 0:
            return x, y, z

        # åº”ç”¨åŠ æƒå¹³å‡
        smooth_x = sum(self.x_history) / len(self.x_history)
        smooth_y = sum(self.y_history) / len(self.y_history)
        smooth_z = sum(self.z_history) / len(self.z_history)

        return smooth_x, smooth_y, smooth_z
    
    def calculate_search_position(self):
        """è®¡ç®—æœç´¢æ¨¡å¼ä¸‹çš„ä½ç½®"""
        current_time = time.time()
        elapsed = current_time - self.search_start_time
        
        phase = elapsed * SEARCH_MOVE_SPEED
        self.search_phase = phase
        
        # æœç´¢æ—¶å·¦å³ç§»åŠ¨
        search_x = SEARCH_MOVE_RANGE * math.sin(phase)
        search_y = 15
        search_z = 15
        
        return search_x, search_y, search_z
    
    def send_to_arm(self, x, y, z):
        """å‘é€åæ ‡åˆ°æœºæ¢°è‡‚"""
        return self.arm_controller.send_command(x, y, z)
    
    def extract_tv_roi(self, frame, boxes):
        """æå–ç”µè§†ROIåŒºåŸŸ"""
        if len(boxes) == 0:
            return None
            
        largest_box = max(boxes, key=lambda box: (box[2]-box[0])*(box[3]-box[1]))
        x1, y1, x2, y2 = [int(coord) for coord in largest_box]
        
        x1 = max(0, x1)
        y1 = max(0, y1)
        x2 = min(frame.shape[1] - 1, x2)
        y2 = min(frame.shape[0] - 1, y2)
        
        if x2 - x1 < 10 or y2 - y1 < 10:
            return None
        
        tv_roi = frame[y1:y2, x1:x2]
        
        if tv_roi.size == 0:
            return None
        
        return tv_roi
    
    def update_screen_status(self):
        """æ›´æ–°å±å¹•çŠ¶æ€"""
        result = self.classification_worker.get_result()
        if result:
            self.screen_status, self.screen_confidence = result
            self.classification_count += 1
    
    def visualize_detections(self, frame, boxes, labels, scores):
        """åœ¨å¸§ä¸Šå¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        im = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(im)
        
        for idx, b in enumerate(boxes):
            x0, y0, x1, y1 = [float(coord) for coord in b]
            
            draw.rectangle([x0, y0, x1, y1], outline='red', width=2)
            
            score = scores[idx]
            text = f"TV: {score:.2f}"
            if self.font:
                draw.text((x0, y0), text=text, fill='red', font=self.font)
            else:
                draw.text((x0, y0), text=text, fill='red')
            
            status_text = f"Status: {self.screen_status}"
            if self.font:
                draw.text((x0, y0 + 20), text=status_text, fill='yellow', font=self.font)
            else:
                draw.text((x0, y0 + 20), text=status_text, fill='yellow')
        
        return cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)
    
    def update_fps(self):
        """æ›´æ–°å¸§ç‡ç»Ÿè®¡"""
        self.frame_count += 1
        if self.frame_count >= 30:
            elapsed = time.time() - self.start_time
            self.avg_fps = self.frame_count / elapsed
            self.frame_count = 0
            self.start_time = time.time()
    
    def process_frame(self, frame):
        """å¤„ç†è§†é¢‘å¸§å¹¶æ£€æµ‹ç”µè§†"""
        self.update_fps()
        
        display_frame = frame.copy()
        
        boxes, labels, scores = self.detector.detect_tv(frame)

        if len(boxes) > 0:
            tv_roi = self.extract_tv_roi(frame, boxes)
            
            if tv_roi is not None:
                try:
                    self.content_changed = self.frame_difference_detector.detect_change(tv_roi)
                    if self.content_changed or self.force_classification:
                        self.classification_worker.submit_task(tv_roi)
                        self.force_classification = False
                except Exception as e:
                    self.frame_difference_detector.prev_frame = None
                    self.content_changed = True
            
            self.update_screen_status()
            
            display_frame = self.visualize_detections(display_frame, boxes, labels, scores)
            
            tv_info = self.detector.calculate_tv_center(boxes, labels)
            
            if tv_info:
                center_x, center_y, tv_w, tv_h = tv_info
                
                cv2.circle(display_frame, (int(center_x), int(center_y)), 10, (0, 0, 255), -1)
                cv2.circle(display_frame, (int(IMAGE_WIDTH/2), int(IMAGE_HEIGHT/2)), 5, (255, 0, 0), -1)
                cv2.line(display_frame, 
                        (int(IMAGE_WIDTH/2), int(IMAGE_HEIGHT/2)), 
                        (int(center_x), int(center_y)), 
                        (255, 255, 0), 2)
                
                new_x, new_y, new_z = self.calculate_target_position(center_x, center_y, tv_w, tv_h)

                if new_x is not None and new_y is not None and new_z is not None:
                    self.target_x, self.target_y, self.target_z = self.smooth_position(
                        new_x, new_y, new_z)

                    self.send_to_arm(self.target_x, self.target_y, self.target_z)

                self.tv_detected = True
                self.last_tv_time = time.time()
                
                if self.search_mode:
                    self.search_mode = False
                    print("ğŸ¯ é€€å‡ºæœç´¢æ¨¡å¼")

                # è®¡ç®—å½“å‰åç§»é‡
                offset_x = center_x - IMAGE_WIDTH/2
                offset_y = center_y - IMAGE_HEIGHT/2
                current_offset = math.sqrt(offset_x**2 + offset_y**2)
                
                # åˆ¤æ–­ç”µè§†ä½ç½®
                if offset_x < 0:
                    x_position = "LEFT"
                else:
                    x_position = "RIGHT"
                    
                if offset_y < 0:
                    y_position = "UP"
                else:
                    y_position = "DOWN"
                
                # æ˜¾ç¤ºä¿¡æ¯
                cv2.putText(display_frame, f"X: {self.target_x:.1f}", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(display_frame, f"Y: {self.target_y:.1f}", (10, 60),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(display_frame, f"Z: {self.target_z:.1f}", (10, 90),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(display_frame, f"TVs: {len(boxes)}", (10, 120),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(display_frame, "CORRECT TRACKING", (10, 150),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                cv2.putText(display_frame, f"Screen: {self.screen_status}", (10, 180),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                cv2.putText(display_frame, f"Confidence: {self.screen_confidence:.2f}", (10, 210),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                
                conn_status = "CONNECTED" if self.arm_controller.connected else "DISCONNECTED"
                cv2.putText(display_frame, f"Arm: {conn_status}", (10, 240),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                
                cv2.putText(display_frame, f"TV Position: {x_position}, {y_position}", (10, 270),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                cv2.putText(display_frame, f"Offset X: {offset_x:.1f}", (10, 300),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                cv2.putText(display_frame, f"Offset Y: {offset_y:.1f}", (10, 330),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                
                # æ˜¾ç¤ºç§»åŠ¨æ–¹å‘
                if new_x is not None:
                    if new_x < self.target_x:
                        move_x = "â† LEFT"
                    else:
                        move_x = "â†’ RIGHT"
                else:
                    move_x = "HOLD"
                    
                if new_z is not None:
                    if new_z < self.target_z:
                        move_z = "â†“ DOWN"
                    else:
                        move_z = "â†‘ UP"
                else:
                    move_z = "HOLD"
                
                cv2.putText(display_frame, f"Move X: {move_x}", (10, 360),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                cv2.putText(display_frame, f"Move Z: {move_z}", (10, 390),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        else:
            self.frame_difference_detector.prev_frame = None
            self.force_classification = True
            
            if self.tv_detected and (time.time() - self.last_tv_time > RESET_WAIT_TIME):
                self.search_mode = True
                self.tv_detected = False
                self.search_start_time = time.time()
                print("ğŸ” è¿›å…¥æœç´¢æ¨¡å¼")
            
            if self.search_mode:
                search_x, search_y, search_z = self.calculate_search_position()
                self.send_to_arm(search_x, search_y, search_z)
                self.target_x, self.target_y, self.target_z = search_x, search_y, search_z
                
                cv2.putText(display_frame, "SEARCHING", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                cv2.putText(display_frame, f"X: {self.target_x:.1f}", (10, 60),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            else:
                if self.tv_detected and (time.time() - self.last_tv_time > 2.0):
                    self.target_x = 0
                    self.target_y = 15
                    self.target_z = 15
                    self.send_to_arm(self.target_x, self.target_y, self.target_z)
                    self.tv_detected = False

                cv2.putText(display_frame, "No TV detected", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        cv2.putText(display_frame, f"FPS: {self.avg_fps:.1f}", (10, IMAGE_HEIGHT - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        return display_frame
    
    def run(self):
        """è¿è¡Œç”µè§†è¿½è¸ª"""
        cap = cv2.VideoCapture(CAMERA_INDEX)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´: {CAMERA_INDEX}")
            return
        
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, IMAGE_WIDTH)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, IMAGE_HEIGHT)
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        print("ğŸš€ å¼€å§‹ç”µè§†è¿½è¸ªï¼ˆä¿®æ­£æ–¹å‘ç‰ˆï¼‰...")
        print("ğŸ›‘ æŒ‰ 'q' é”®é€€å‡º")
        print("ğŸ”§ æŒ‰ 'r' é”®é‡ç½®æœºæ¢°è‡‚ä½ç½®")
        print("ğŸ”Œ æŒ‰ 'd' é”®é‡æ–°è¿æ¥æœºæ¢°è‡‚")
        print("ğŸ“Š æŒ‰ 't' é”®æµ‹è¯•ç§»åŠ¨æ–¹å‘")
        
        while self.running:
            ret, frame = cap.read()
            if not ret:
                print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                break
            
            if len(frame.shape) == 2:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
            
            if FLIP_CAMERA:
                frame = cv2.flip(frame, 1)
            
            processed_frame = self.process_frame(frame)
            
            cv2.imshow('TV Tracking - Correct Direction', processed_frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('r'):
                self.target_x = 0
                self.target_y = 15
                self.target_z = 15
                self.send_to_arm(self.target_x, self.target_y, self.target_z)
                print("ğŸ”„ é‡ç½®æœºæ¢°è‡‚ä½ç½®")
            elif key == ord('d'):
                self.arm_controller.connect_to_server()
                print("ğŸ”„ å°è¯•é‡æ–°è¿æ¥æœºæ¢°è‡‚")
            elif key == ord('t'):
                # æµ‹è¯•ç§»åŠ¨æ–¹å‘
                print("ğŸ§ª å¼€å§‹æ–¹å‘æµ‹è¯•...")
                print("1. å‘å·¦ç§»åŠ¨ (x = -10)")
                self.send_to_arm(-10, 15, 15)
                time.sleep(1)
                print("2. å‘å³ç§»åŠ¨ (x = 10)")
                self.send_to_arm(10, 15, 15)
                time.sleep(1)
                print("3. å‘ä¸Šç§»åŠ¨ (z = 18)")
                self.send_to_arm(0, 15, 18)
                time.sleep(1)
                print("4. å‘ä¸‹ç§»åŠ¨ (z = 12)")
                self.send_to_arm(0, 15, 12)
                time.sleep(1)
                print("5. è¿”å›ä¸­å¿ƒ")
                self.send_to_arm(0, 15, 15)
                print("âœ… æ–¹å‘æµ‹è¯•å®Œæˆ")
        
        cap.release()
        cv2.destroyAllWindows()
        self.classification_worker.stop()
        self.arm_controller.stop()
        print("ğŸ›‘ ç”µè§†è¿½è¸ªå·²åœæ­¢")


if __name__ == '__main__':
    print("=" * 50)
    print("ç”µè§†å±å¹•è¿½è¸ªå®¢æˆ·ç«¯ - ä¿®æ­£æ–¹å‘ç‰ˆæœ¬")
    print("=" * 50)
    print("- ç”µè§†å¤ªå¤§ â†’ åé€€ (yå¢åŠ )")
    print("- ç”µè§†å¤ªå° â†’ å‰è¿› (yå‡å°‘)")
    print("=" * 50)
    
    tracker = TVTracker(MODEL_PATH, SERVER_IP, SERVER_PORT, CLASSIFICATION_WEIGHTS)
    tracker.run()